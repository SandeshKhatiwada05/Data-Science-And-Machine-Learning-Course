What is Data Science?

Field that uses statistics, programming, and domain knowledge to extract insights from data.

Combines:

Math & Stats → for analysis

Programming (Python/R) → for automation & modeling

Business/domain understanding → for meaningful interpretation

Typical Workflow:

Data Collection

Data Cleaning

Exploratory Data Analysis (EDA)

Feature Engineering

Model Building

Evaluation & Deployment

Common Python Libraries:

NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn








Data Cleaning & Exploration

Data Cleaning:

Handle missing values → dropna(), fillna()

Handle duplicates → drop_duplicates()

Fix data types → astype()

Handle outliers → IQR or Z-score methods

Exploratory Data Analysis (EDA):

Understand data patterns, correlations, and distributions.

Tools:

.describe() → summary stats

.info() → data structure

sns.heatmap(df.corr()) → correlation check

sns.pairplot(df) → feature relationships





Feature Engineering & Data Preprocessing

Feature Engineering:

Creating new features from existing ones to improve model performance.

Example:

From “Date” → extract “Year”, “Month”, “Day”

Combine multiple columns → ratios, sums, or interactions

Data Preprocessing:

Encoding Categorical Data:

Label Encoding → LabelEncoder()

One-Hot Encoding → pd.get_dummies()

Scaling Features:

Standardization → StandardScaler()

Normalization → MinMaxScaler()

Train-Test Split:





Model Evaluation & Improvement
Model Evaluation Metrics:

Regression: MAE, MSE, RMSE, R²

Classification: Accuracy, Precision, Recall, F1-score, ROC-AUC

Cross-Validation:

Reduces overfitting risk

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
Model Tuning:

Grid Search: tests multiple hyperparameter combinations

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(model, param_grid, cv=5)
grid.fit(X_train, y_train)
Random Search: faster alternative

Overfitting Fixes:

Simplify model

More data

Regularization (L1, L2)

Dropout (for neural nets)






Linear Regression

Predicts continuous values based on linear relationships between variables.

Equation: Y = b0 + b1X1 + b2X2 + ... + bnXn

Goal: Minimize prediction error using Mean Squared Error (MSE).

Assumptions:

Linearity

Independence of errors

Homoscedasticity

Normality of residuals

No multicollinearity

Evaluation metrics: MSE, RMSE, R² score

Advantages: Simple, interpretable, fast.

Disadvantages: Poor for non-linear data, sensitive to outliers.

Use cases: Price prediction, sales forecasting, trend estimation.







Logistic Regression

Used for classification, not regression.

Predicts probability of an outcome (e.g., 0 or 1).

Output threshold:

≥ 0.5 → Class 1

< 0.5 → Class 0

Types:

Binary (yes/no)

Multinomial (multi-class)

Ordinal (ranked classes)

Evaluation metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC

Assumptions:

Linear relationship between features and log-odds

Independent observations

No multicollinearity

Advantages: Simple, interpretable, efficient.

Disadvantages: Can’t capture non-linear patterns.

Use cases: Spam detection, disease prediction, customer churn analysis.




  

Decision Tree

A supervised learning algorithm used for classification and regression.

Splits data into branches based on feature conditions → forms a tree structure.

Key terms:

Root Node: First split of the data

Decision Node: Intermediate split

Leaf Node: Final output/prediction

Splitting Criteria:

Classification → Gini Impurity, Entropy (Information Gain)

Regression → Mean Squared Error (MSE)

Stopping conditions:

Max depth

Minimum samples per leaf

Pure node (all same class)

Advantages: Easy to interpret, no scaling needed, handles non-linear data.

Disadvantages: Prone to overfitting, unstable (small data changes can alter tree).

Solutions: Use Random Forest or Pruning.

Use cases: Credit risk prediction, medical diagnosis, loan approval.


