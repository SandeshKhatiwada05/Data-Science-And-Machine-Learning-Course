What is Data Science?

Field that uses statistics, programming, and domain knowledge to extract insights from data.

Combines:

Math & Stats → for analysis

Programming (Python/R) → for automation & modeling

Business/domain understanding → for meaningful interpretation

Typical Workflow:

Data Collection

Data Cleaning

Exploratory Data Analysis (EDA)

Feature Engineering

Model Building

Evaluation & Deployment

Common Python Libraries:

NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn








Data Cleaning & Exploration

Data Cleaning:

Handle missing values → dropna(), fillna()

Handle duplicates → drop_duplicates()

Fix data types → astype()

Handle outliers → IQR or Z-score methods

Exploratory Data Analysis (EDA):

Understand data patterns, correlations, and distributions.

Tools:

.describe() → summary stats

.info() → data structure

sns.heatmap(df.corr()) → correlation check

sns.pairplot(df) → feature relationships





Feature Engineering & Data Preprocessing

Feature Engineering:

Creating new features from existing ones to improve model performance.

Example:

From “Date” → extract “Year”, “Month”, “Day”

Combine multiple columns → ratios, sums, or interactions

Data Preprocessing:

Encoding Categorical Data:

Label Encoding → LabelEncoder()

One-Hot Encoding → pd.get_dummies()

Scaling Features:

Standardization → StandardScaler()

Normalization → MinMaxScaler()

Train-Test Split:





Model Evaluation & Improvement
Model Evaluation Metrics:

Regression: MAE, MSE, RMSE, R²

Classification: Accuracy, Precision, Recall, F1-score, ROC-AUC

Cross-Validation:

Reduces overfitting risk

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
Model Tuning:

Grid Search: tests multiple hyperparameter combinations

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(model, param_grid, cv=5)
grid.fit(X_train, y_train)
Random Search: faster alternative

Overfitting Fixes:

Simplify model

More data

Regularization (L1, L2)

Dropout (for neural nets)






Linear Regression

Predicts continuous values based on linear relationships between variables.

Equation: Y = b0 + b1X1 + b2X2 + ... + bnXn

Goal: Minimize prediction error using Mean Squared Error (MSE).

Assumptions:

Linearity

Independence of errors

Homoscedasticity

Normality of residuals

No multicollinearity

Evaluation metrics: MSE, RMSE, R² score

Advantages: Simple, interpretable, fast.

Disadvantages: Poor for non-linear data, sensitive to outliers.

Use cases: Price prediction, sales forecasting, trend estimation.
